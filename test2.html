<!-- <!DOCTYPE html> -->
<html>
<head>
	<meta charset='utf-8'/>
	<meta name="viewport" content="width=device-width, initial-scale=1"/>
	<title>Sense | Integrated Emotion Detection </title>
	<link rel='stylesheet' href='https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css'/>
	<link rel='stylesheet' href='css/style.css'/>
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
	<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,500" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700" rel="stylesheet">
	

	<script>
		// getUserMedia only works over https in Chrome 47+, so we redirect to https. Also notify user if running from file.
		if (window.location.protocol == "file:") {
			alert("You seem to be running this example directly from a file. Note that these examples only work when served from a server or localhost due to canvas cross-domain restrictions.");
		} else if (window.location.hostname !== "localhost" && window.location.protocol !== "https:"){
			window.location.protocol = "https";
		}
	</script>
		
</head>
<body>

	<script src="js/utils.js"></script>
	<script src="js/clmtrackr.js"></script>
	<script src="models/model_pca_20_svm_emotionDetection.js"></script>
	<script src="js/Stats.js"></script>
	<script src="js/d3.min.js"></script>
	<script src="js/emotion_classifier.js"></script>
	<script src="js/emotionmodel.js"></script>

	<script src="js/webgazer.js" type="text/javascript"></script>


<div id="leftpanel">
	<!-- Collects the emojis that we annotate a page with -->
	<div id="emojiAnnotations"></div>

	<div class="title">User Test: Emotion Recognition</div>
	<div class="contentText">
		<div class="h3" id="prompt">happy</div>

		<p>We display a prompt, above, for an emotion on the screen. Make that expression at your screen and press spacebar. Once, you press spacebar, the prompt will advance to the next one, and you will repeat the process.</p>

		<p>When complete, the data will automatically download.</p>
		<br><Br>
		<p><a href="test.html">Click here to go to the first user study!</a></p>	
	</div>
</div>


<div id="rightpanel">

	<img src="img/logo.png" class="logo" />

	<div id="content">
		<div id="controls">
			<input class="btn" type="button" value="wait, loading video" disabled="disabled" onclick="startVideo()" id="startbutton"></input>
		</div>
		<br>
		

		<div id="container">
			<video id="videoel" width="400" height="300" preload="auto" loop>
			</video>
			<canvas id="overlay" width="400" height="300"></canvas>
		</div>
	</div>
</div>


<!-- Shows where the smoothed gaze position is -->
<div id="eyePosPrediction"></div>




	




		<script>	
//===========================================================================
//===========================================================================
//            g a z e    t r a c k i n g   u s e r   t e s t
//===========================================================================
//===========================================================================
// make face, click spacebar
// put data in js object, etc.


function download(text, name, type) {
    var a = document.createElement("a");
    var file = new Blob([text], {type: type});
    a.href = URL.createObjectURL(file);
    a.download = name;
    a.click();
}

// Test 1-10
// var EMOTION_PROMPTS = ["happy","sad","suprised","angry","happy","sad","happy","surprised","angry","sad","happy","sad","surprised","happy","surprised","angry","happy","surprised","sad","happy"]
var emotion_prompts = ["happy"]
var emotion_results = []
var current_test = 0
var total_tests = 10;

$("#prompt").text(emotion_prompts[0]);

var correct_happy = 0;
var correct_sad = 0;
var correct_angry = 0;
var correct_surprised = 0;

var EMOTION_OPTIONS = ["happy","sad","surprised","angry"]

document.body.onkeyup = function(e){
    if(e.keyCode == 32){
    	if (current_test < total_tests) {
    		guess = currentGuess;
    		emotion_results.push(guess);
    		current_test += 1;

			if (current_test != total_tests){
	    		nextTest = EMOTION_OPTIONS[Math.floor(Math.random()*4)];
	    		emotion_prompts.push(nextTest);
	    		$("#prompt").text(nextTest);
		 	}

		 	if (guess != emotion_prompts[current_test-1]){
		 		if (guess=="happy"){
		 			correct_happy +=1;
		 		}
		 		else if (guess =="sad"){
		 			correct_sad += 1;
		 		}
		 		else if (guess == "surprised") {
		 			correct_surprised += 1;
		 		}
		 		else {
		 			correct_angry +=1;
		 		}
		 	}
			else {
				result = { };
				for(var i = 0; i < emotion_prompts.length; ++i) {
				    if(!result[emotion_prompts[i]])
				        result[emotion_prompts[i]] = 0;
				    ++result[emotion_prompts[i]];
				}
				var emotionData = {};
				emotionData["correctHappy"] = correct_happy/result["happy"];
				emotionData["correctSad"] = correct_sad/result["sad"];
				emotionData["correctSurprised"] = correct_surprised/result["surprised"];
				emotionData["correctAngry"] = correct_angry/result["angry"];
				emotionData["totalCorrect"] = (correct_surprised + correct_angry + correct_sad + correct_happy)/total_tests
				
				file_text = JSON.stringify(emotionData);
				download(file_text, "emotionTest.txt", 'text/plain')
			}
		}
    }
}


var currentHappy = 0;
var currentSad = 0;
var currentAngry = 0;
var currentSurprised = 0;
var currentGuess = 0


function updateEmotionData(happy, sad, angry, surprised, guess){
	currentHappy = happy;
	currentSad = sad;
	currentAngry = angry;
	currentSurprised = surprised;
	currentGuess = guess;
}




//===========================================================================
//===========================================================================
//                        g a z e  t r a c k i n g
//===========================================================================
//===========================================================================

		var WINDOW_SIZE = 20;
		var EMOTION_TIMEOUT_MINIMUM = 200;
		var EMOTION_WINDOW_SIZE = 100;

		// Gaze Tracking constants and counters
		var gazeX = Array(WINDOW_SIZE).fill(0);
		var gazeY = Array(WINDOW_SIZE).fill(0);
		var currentX = 20;
		var currentY = 20;

		// Emotion window smoothing
		var happyWindow = Array(EMOTION_WINDOW_SIZE).fill(0);
		var sadWindow = Array(EMOTION_WINDOW_SIZE).fill(0);
		var surprisedWindow = Array(EMOTION_WINDOW_SIZE).fill(0);
		var angryWindow = Array(EMOTION_WINDOW_SIZE).fill(0);



		// Takes in the coordinates from WebGazer, adjusts them to be relative to viewports
		// and screen position, and then add our value to the smoothing protocol
		function updateGazePoints(x, y) {

			// Collect the scroll position because gaze tracking only gives us relative information based
			// on what is displayed on the page window
			// reference: http://stackoverflow.com/questions/3464876/javascript-get-window-x-y-position-for-scroll
			var topScrollOffset  = $("#leftpanel").scrollTop();
		    // var leftScrollOffset = window.pageXOffset || document.documentElement.scrollLeft;
		    var leftScrollOffset = 0;

			var xpred = x + leftScrollOffset; //these x coordinates are relative to the viewport 
		    var ypred = y + topScrollOffset; //these y coordinates are relative to the viewport

		    gazeX.pop();
		    gazeY.pop();
		    gazeX.unshift(xpred);
		    gazeY.unshift(ypred);

		    // average over the window to reduce noise, can apply a weighting function to it
			currentX = gazeX.reduce( (prev, curr) => prev + curr )/(WINDOW_SIZE);
			currentY = gazeY.reduce( (prev, curr) => prev + curr )/(WINDOW_SIZE);

			// display the results
		    // console.log(currentX, currentY);
			

		}

		


		// Set regression model used in tracking
		webgazer.setRegression('weightedRidge');

		// Collect prediction values
		webgazer.setGazeListener(function(data, elapsedTime) {
		    if (data == null) {
		        return;
		    }

		    // Bound prediction by viewport edges
		    webgazer.util.bound(data);
		    var xprediction = data.x; // These x coordinates are relative to the viewport 
		    var yprediction = data.y; // These y coordinates are relative to the viewport

		    // Add to window for smoothing
		    updateGazePoints(xprediction, yprediction);
		    
		}).begin()
		.showPredictionPoints(false);
		           


//===========================================================================
//===========================================================================
//            e m o t i o n    r e c o g n i t i o n 
//===========================================================================
//===========================================================================

		var vid = document.getElementById('videoel');
		// var overlay = document.getElementById('overlay');
		// var overlayCC = overlay.getContext('2d');
		
		/********** check and set up video/webcam **********/

		function enablestart() {
			var startbutton = document.getElementById('startbutton');
			startbutton.value = "start";
			startbutton.disabled = null;
		}
		
		navigator.getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.mozGetUserMedia || navigator.msGetUserMedia;
		window.URL = window.URL || window.webkitURL || window.msURL || window.mozURL;

		// check for camerasupport
		if (navigator.getUserMedia) {
			// set up stream
			
			var videoSelector = {video : true};
			if (window.navigator.appVersion.match(/Chrome\/(.*?) /)) {
				var chromeVersion = parseInt(window.navigator.appVersion.match(/Chrome\/(\d+)\./)[1], 10);
				if (chromeVersion < 20) {
					videoSelector = "video";
				}
			};
		
			navigator.getUserMedia(videoSelector, function( stream ) {
				if (vid.mozCaptureStream) {
					vid.mozSrcObject = stream;
				} else {
					vid.src = (window.URL && window.URL.createObjectURL(stream)) || stream;
				}
				vid.play();
			}, function() {
				//insertAltVideo(vid);
				alert("There was some problem trying to fetch video from your webcam. If you have a webcam, please make sure to accept when the browser asks for access to your webcam.");
			});
		} else {
			//insertAltVideo(vid);
			alert("This demo depends on getUserMedia, which your browser does not seem to support. :(");
		}

		vid.addEventListener('canplay', enablestart, false);
		
		// setup of emotion detection
		var ctrack2 = new clm.tracker({useWebGL : true});
		ctrack2.init(pModel);

		function startVideo() {
			// start video
			vid.play();
			// start tracking
			ctrack2.start(vid);
			// start loop to draw face
			drawLoop();
		}
		
		initial = [0,0,0,0]
		initial_window = 0
		INITIAL_WINDOW_SIZE = 1

		function drawLoop() {
			requestAnimFrame(drawLoop);
			var cp = ctrack2.getCurrentParameters();
			var er = ec.meanPredict(cp);
			if (er) {
				// Generate a baseline average so that 
				// Calibration sequence
				if (initial_window < INITIAL_WINDOW_SIZE){
					for (var i = 0;i < er.length;i++) {
						initial[i] += er[i].value/INITIAL_WINDOW_SIZE;
						initial_window += 1;
					}
				}
				// Actually find the emotion that is being recorded
				else {
					maxEmotion = "neutral";
					sad = 0;
					happy = 0;
					angry = 0;
					surprised = 0;

					maxValue = 0;
					for (var i = 0;i < er.length;i++) {
						// if (  (maxValue < (er[i].value-initial[i])/initial[i]) || er[i].value > 0.6 ){
						if ( maxValue < er[i].value ){
							maxEmotion = er[i].emotion;
							maxValue = er[i].value;
						}

						if (er[i].emotion == "happy"){
							// happy = Math.max((er[i].value - initial[i])/er[i].value, er[i].value);
							happy = er[i].value * 4;
							happyWindow.pop();
						    happyWindow.unshift(happy);
						}

						if (er[i].emotion == "sad"){
							sad = er[i].value;
							sadWindow.pop();
						    sadWindow.unshift(sad);
							// sad =  Math.max((er[i].value - initial[i])/er[i].value, er[i].value);
						}

						if (er[i].emotion == "angry"){
							angry  = er[i].value;
							angryWindow.pop();
						    angryWindow.unshift(angry);
							// =  Math.max((er[i].value - initial[i])/er[i].value, er[i].value);
						}

						if (er[i].emotion == "surprised"){
							surprised = er[i].value;
							surprisedWindow.pop();
						    surprisedWindow.unshift(surprised);
							 // =  Math.max((er[i].value - initial[i])/er[i].value, er[i].value);
						}

					}

					console.log(maxEmotion)
					
					// make this the default emotion that is displayed
					maxEmotion = "neutral"

					// average over the window to reduce noise, can apply a weighting function to it
					happy = happyWindow.reduce( (prev, curr) => prev + curr )/(EMOTION_WINDOW_SIZE);
					sad = sadWindow.reduce( (prev, curr) => prev + curr )/(EMOTION_WINDOW_SIZE);
					angry = angryWindow.reduce( (prev, curr) => prev + curr )/(EMOTION_WINDOW_SIZE);
					surprised = surprisedWindow.reduce( (prev, curr) => prev + curr )/(EMOTION_WINDOW_SIZE);

					// a threshold cascade to see if the emotion is extreme enough to
					// characterize with certainty, and then display
					if (sad < 0.1){ // cascaded because both of these are less sensitive and need to be 				   boosted if they are expressed
						if (angry > 0.4){
							maxEmotion = "angry";
							
						}
						else if (angry < 0.1) {
							maxEmotion = "sad";
							
						}
					}
					else if (surprised > 0.2){
						maxEmotion = "surprised";
						
					}
					else if (happy > 0.02) {
						maxEmotion = "happy";
						
					}
					else {
						maxEmotion = "neutral";
					}

					updateEmotionData(happy, sad, angry, surprised, maxEmotion);
				}
			}
		}
		
		var ec = new emotionClassifier();
		ec.init(emotionModel);
		var emotionData = ec.getBlank();			
	</script>


</body>
</html>