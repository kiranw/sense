<!-- <!DOCTYPE html> -->
<html>
<head>
	<meta charset='utf-8'/>
	<meta name="viewport" content="width=device-width, initial-scale=1"/>
	<title>Sense | Integrated Emotion Detection </title>
	<link rel='stylesheet' href='https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css'/>
	<link rel='stylesheet' href='css/style.css'/>
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
	<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,500" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700" rel="stylesheet">
	

	<script>
		// getUserMedia only works over https in Chrome 47+, so we redirect to https. Also notify user if running from file.
		if (window.location.protocol == "file:") {
			alert("You seem to be running this example directly from a file. Note that these examples only work when served from a server or localhost due to canvas cross-domain restrictions.");
		} else if (window.location.hostname !== "localhost" && window.location.protocol !== "https:"){
			window.location.protocol = "https";
		}
	</script>
		
</head>
<body>

	<script src="js/utils.js"></script>
	<script src="js/clmtrackr.js"></script>
	<script src="models/model_pca_20_svm_emotionDetection.js"></script>
	<script src="js/Stats.js"></script>
	<script src="js/d3.min.js"></script>
	<script src="js/emotion_classifier.js"></script>
	<script src="js/emotionmodel.js"></script>
	<script src="js/plotly-latest.min.js"></script>
	<script src="js/webgazer.js" type="text/javascript"></script>


<div id="leftpanel">
	<!-- Collects the emojis that we annotate a page with -->
	<div id="emojiAnnotations"></div>

	<div class="title">Sense, automated localized emotion tagging for web content</div>
	<div class="contentText">
		<p>Sense is a project we developed for 6.835, in the Spring of 2017. Its a simple platform that overlays emojis over web content when a transition in emotion is detected, at the position of estimated gaze during that transition. Using some implementation built over CLM-tracker, we are able to map a face template to a face, and use measured deformation of different points to measure happiness, sadness, surprise and anger. When an emotion's prediction is strong enough and passes a threshold, it triggers a process that then collects the current gaze estimation, displays the emotion if it meets requirements and collects the data for analysis.</p>

		<p>We're going to go through a few different examples of text and images that will serve as a playground for you to express emotions. This system works best with extreme displays of emotion, which may not be natural for most content you view, but the aim is to tune the system to work on more subtle emotions in the future when the system comes with more calibration that is custom to how you specifically express emotions (this will adjust for sensitivity in subtle displays of emotion).</p>

		<p>Sense is a project we developed for 6.835, in the Spring of 2017. Its a simple platform that overlays emojis over web content when a transition in emotion is detected, at the position of estimated gaze during that transition. Using some implementation built over CLM-tracker, we are able to map a face template to a face, and use measured deformation of different points to measure happiness, sadness, surprise and anger. When an emotion's prediction is strong enough and passes a threshold, it triggers a process that then collects the current gaze estimation, displays the emotion if it meets requirements and collects the data for analysis.</p>

		<p>We're going to go through a few different examples of text and images that will serve as a playground for you to express emotions. This system works best with extreme displays of emotion, which may not be natural for most content you view, but the aim is to tune the system to work on more subtle emotions in the future when the system comes with more calibration that is custom to how you specifically express emotions (this will adjust for sensitivity in subtle displays of emotion).</p>

		<p>Sense is a project we developed for 6.835, in the Spring of 2017. Its a simple platform that overlays emojis over web content when a transition in emotion is detected, at the position of estimated gaze during that transition. Using some implementation built over CLM-tracker, we are able to map a face template to a face, and use measured deformation of different points to measure happiness, sadness, surprise and anger. When an emotion's prediction is strong enough and passes a threshold, it triggers a process that then collects the current gaze estimation, displays the emotion if it meets requirements and collects the data for analysis.</p>

		<p>We're going to go through a few different examples of text and images that will serve as a playground for you to express emotions. This system works best with extreme displays of emotion, which may not be natural for most content you view, but the aim is to tune the system to work on more subtle emotions in the future when the system comes with more calibration that is custom to how you specifically express emotions (this will adjust for sensitivity in subtle displays of emotion).</p>
		<!-- <img class="emotionalContent" src="emotionalMaterial/image1.jpg" /> -->

		<p></p>

	</div>
</div>

<div id="rightpanel">

	<img src="img/logo.png" class="logo" />

	<div id="descriptionBlurb"> Welcome to Sense! When you're ready, press start. You're camera's input will be used to detect emotions and plot them where our system predicts you are looking so you can annotate a page with your reactions, naturally - without any work on your part.</div>

	<div id="content">
		<div id="controls">
			<input class="btn" type="button" value="wait, loading video" disabled="disabled" onclick="startVideo()" id="startbutton"></input>
			
		</div><br>
		<hr>
		<br>
		<div id="buttons">
			<input class="btn" type="button" value="Toggle Emojis" onclick="toggleEmojis()"></input>
			<input class="btn" type="button" data-toggle="modal" data-target="#dataAnalysisModal" value="Display Analysis"  onclick="displayAnalysis()"></input>
			<input class="btn" type="button" value="Final Paper" onclick="downloadPaper()"></input>
		</div>
		<br>
		<img id="emoji" src="img/neutral.png" />
		<br>

		<div class="emotion">	
			You're feeling <span class="emotionvalue">neutral</span>.<br>
		</div>




		<div class="emotionScores">
		Happy: <span class="happyvalue">0.5</span><br>
		Sad: <span class="sadvalue">0.5</span><br>
		Surprised: <span class="surprisedvalue">0.5</span><br>
		Angry: <span class="angryvalue">0.5</span><br>
		</div>

		<div id="container">
			<video id="videoel" width="400" height="300" preload="auto" loop>
			</video>
			<canvas id="overlay" width="400" height="300"></canvas>
		</div>
	</div>
</div>


<!-- Shows where the smoothed gaze position is -->
<!-- <div id="eyePosPrediction"></div> -->


<!-- Modal -->
<div class="modal fade" id="dataAnalysisModal" role="dialog">
<div class="modal-dialog">

  <!-- Modal content-->
  <div class="modal-content">
    <div class="modal-header">
      <button type="button" class="close" data-dismiss="modal">&times;</button>
      <h4 class="modal-title"><img src="img/analysis.png" class="logo2"/></h4>
    </div>
    <div class="modal-body">
      <p>Here's a summary of the emotions we were able to detect on this page.</p>
      <div id="plot-container">
	      <div id="plot1"></div>
	      <div id="plot2"></div>
      </div>
    </div>
    <div class="modal-footer">
      <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
    </div>
  </div>
  
</div>
</div>

	




		<script>	

//===========================================================================
//===========================================================================
//                        g a z e  t r a c k i n g
//===========================================================================
//===========================================================================

		var WINDOW_SIZE = 20;
		var EMOTION_TIMEOUT_MINIMUM = 100;
		var EMOTION_WINDOW_SIZE = 50;

		// Gaze Tracking constants and counters
		var gazeX = Array(WINDOW_SIZE).fill(0);
		var gazeY = Array(WINDOW_SIZE).fill(0);
		var currentX = 20;
		var currentY = 20;

		// Emotion window smoothing
		var happyWindow = Array(EMOTION_WINDOW_SIZE).fill(0);
		var sadWindow = Array(EMOTION_WINDOW_SIZE).fill(0);
		var surprisedWindow = Array(EMOTION_WINDOW_SIZE).fill(0);
		var angryWindow = Array(EMOTION_WINDOW_SIZE).fill(0);

		// Emotion Registering constants and counters
		var lastPlottedX = -500;
		var lastPlottedY = -500;
		var lastEmotion = 0;
		var emotionTimeout = 0;


		// Store coordinates of emojis displayed
		var emojis = { "happyEmojis": [], "sadEmojis": [], "surprisedEmojis": [], "angryEmojis": []}

		// Takes in the coordinates from WebGazer, adjusts them to be relative to viewports
		// and screen position, and then add our value to the smoothing protocol
		function updateGazePoints(x, y) {

			// Collect the scroll position because gaze tracking only gives us relative information based
			// on what is displayed on the page window
			// reference: http://stackoverflow.com/questions/3464876/javascript-get-window-x-y-position-for-scroll
			var topScrollOffset  = $("#leftpanel").scrollTop();
		    // var leftScrollOffset = window.pageXOffset || document.documentElement.scrollLeft;
		    var leftScrollOffset = 0;

			var xpred = x + leftScrollOffset; //these x coordinates are relative to the viewport 
		    var ypred = y + topScrollOffset; //these y coordinates are relative to the viewport

		    gazeX.pop();
		    gazeY.pop();
		    gazeX.unshift(xpred);
		    gazeY.unshift(ypred);

		    // average over the window to reduce noise, can apply a weighting function to it
			currentX = gazeX.reduce( (prev, curr) => prev + curr )/(WINDOW_SIZE);
			currentY = gazeY.reduce( (prev, curr) => prev + curr )/(WINDOW_SIZE);

			// display the results
		    // console.log(currentX, currentY);
			
		 //    $('#eyePosPrediction').css({
			//     transform: 'translate3d(' + xpred + 'px,' + ypred + 'px,0)'
			// })
		}

		// Register an emotion when triggered 
		function registerEmotion(emotion, force=false) {
			console.log("attempting to register emotion")
			// prevent emotions from registering too often
			if (emotionTimeout > EMOTION_TIMEOUT_MINIMUM || force){
				if (Math.abs(lastPlottedX - currentX) > 100 || Math.abs(lastPlottedY - currentY) > 100 || force) {
					// prevent redundant emotions, only plot on transitions
					if (lastEmotion != emotion || force) {
						plotX = currentX;
						plotY = currentY;
						// console.log("plotting emotion")
						$("#emojiAnnotations").append("<img src='img/" + emotion +
													 "_emoji.png' style='" + 
													 "transform:translate3d(" + plotX + 
													 "px," + plotY + "px,0)" + 
													 "' class='emoji'></div>")	
						emojis[emotion + "Emojis"].push(plotY);
						if (!force) {
							lastPlottedX = plotX;
							lastPlottedY = plotY;
							lastEmotion  = emotion;
							emotionTimeout = 0;
						}
					}
				}
			}
			else {
				emotionTimeout += 1;
			}
		}

		

		// Keypress emoji to wherever gaze currently as an alternative input
		document.body.onkeyup = function(e){
		    if(e.keyCode == 41){  // ")"
		    	console.log("force register");
		    	registerEmotion("happy",true);
		    }
		    if(e.keyCode == 40){  // "("
		    	console.log("force register");
		    	registerEmotion("sad",true);
			}
		    if(e.keyCode == 79){ // "O"
		    	console.log("force register");
		    	registerEmotion("surprised",true);
		    }
		    if(e.keyCode == 60){ // "<"
		    	console.log("force register");
		    	registerEmotion("angry",true);
		    }
		}

		function toggleEmojis() {
			$("#emojiAnnotations").toggle();
		}


		function displayAnalysis(){
			emojis = {"happyEmojis":[203.75,1074.85,389.15],"sadEmojis":[715.15],"surprisedEmojis":[280.35],"angryEmojis":[492.6,780.2,1317.75]}
			// Histogram of all emojis
			allEmojis = emojis["happyEmojis"].concat(emojis["sadEmojis"]).concat(emojis["surprisedEmojs"]).concat(emojis["angryEmojis"])
			
			var trace1 = {
			  y: allEmojis,
			  // x: y1,
			  name: 'all',
			  autobinx: false, 
			  histnorm: "count", 
			  marker: {
			    color: "rgba(255, 100, 102, 0.7)", 
			     line: {
			      color:  "rgba(255, 100, 102, 1)", 
			      width: 1
			    }
			  },  
			  opacity: 0.5, 
			  type: "histogram", 
			  xbins: {
			  //   end: 2.8, 
			    size: 0.06, 
			  //   start: .5
			  }
			};
			var trace2 = {
			  y: emojis["happyEmojis"],
			  // x: y2, 
			  autobinx: false, 
			  marker: {
			          color: "rgba(222, 170, 0, 0.7)",
			           line: {
			            color:  "rgba(222, 170, 0, 1)", 
			            width: 1
			    } 
			       }, 
			  name: "happy", 
			  opacity: 0.75, 
			  type: "histogram", 
			  xbins: { 
			    // end: 4, 
			    size: 0.06, 
			    // start: -3.2
			  }
			};
			var trace3 = {
			  y: emojis["sadEmojis"],
			  // x: y2, 
			  autobinx: false, 
			  marker: {
			          color: "rgba(57, 122, 127, 0.7)",
			           line: {
			            color:  "rgba(57, 122, 127, 1)", 
			            width: 1
			    } 
			       }, 
			  name: "sad", 
			  opacity: 0.75, 
			  type: "histogram", 
			  ybins: { 
			    // end: 4, 
			    size: 0.06, 
			    // start: -3.2
			  }
			};
			var trace4 = {
			  y: emojis["surprisedEmojis"],
			  // x: y2, 
			  autobinx: false, 
			  marker: {
			          color: "rgba(96, 230, 150, 0.7)",
			           line: {
			            color:  "rgba(96, 230, 150, 1)", 
			            width: 1
			    } 
			       }, 
			  name: "surprised", 
			  opacity: 0.75, 
			  type: "histogram", 
			  ybins: { 
			    // end: 4, 
			    size: 0.06, 
			    // start: -3.2
			  }
			};
			var trace5 = {
			  y: emojis["angryEmojis"],
			  // x: y2, 
			  autobinx: false, 
			  marker: {
			          color: "rgba(205, 49, 53, 0.7)",
			           line: {
			            color:  "rgba(205, 49, 53, 1)", 
			            width: 1
			    } 
			       }, 
			  name: "angry", 
			  opacity: 0.75, 
			  type: "histogram", 
			  ybins: { 
			    // end: 4, 
			    size: 0.06, 
			    // start: -3.2
			  }
			};
			var data = [trace2, trace3, trace4, trace5];
			var layout = {
			  bargap: 0.05, 
			  bargroupgap: 0.2, 
			  barmode: "stack", 
			  title: "Mixed Emojis vs Vertical Height", 
			  xaxis: {title: "Count"}, 
			  yaxis: {title: "Scroll Position"}
			};
			Plotly.newPlot('plot', data, layout);

			var data2 = [trace1];
			var layout = {
			  bargap: 0.05, 
			  bargroupgap: 0.2, 
			  barmode: "stack", 
			  title: "All Emojis vs Vertical Height", 
			  xaxis: {title: "Count"}, 
			  yaxis: {title: "Scroll Position"}
			};
			Plotly.newPlot('plot', data2, layout);








			// Histogram of happy emojis


			// Histogram of sad emojis


			// Histogram of surprised emojis


			// Bubble chart / heatmap?


			// Most common emotion measured:


			// Least common emotion measured:

		}


		// Set regression model used in tracking
		webgazer.setRegression('weightedRidge');

		// Collect prediction values
		webgazer.setGazeListener(function(data, elapsedTime) {
		    if (data == null) {
		        return;
		    }

		    // Bound prediction by viewport edges
		    webgazer.util.bound(data);
		    var xprediction = data.x; // These x coordinates are relative to the viewport 
		    var yprediction = data.y; // These y coordinates are relative to the viewport

		    // Add to window for smoothing
		    updateGazePoints(xprediction, yprediction);
		    
		}).begin()
		.showPredictionPoints(false);
		           


//===========================================================================
//===========================================================================
//            e m o t i o n    r e c o g n i t i o n 
//===========================================================================
//===========================================================================

		var vid = document.getElementById('videoel');
		// var overlay = document.getElementById('overlay');
		// var overlayCC = overlay.getContext('2d');
		
		/********** check and set up video/webcam **********/

		function enablestart() {
			var startbutton = document.getElementById('startbutton');
			startbutton.value = "start";
			startbutton.disabled = null;
		}
		
		navigator.getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.mozGetUserMedia || navigator.msGetUserMedia;
		window.URL = window.URL || window.webkitURL || window.msURL || window.mozURL;

		// check for camerasupport
		if (navigator.getUserMedia) {
			// set up stream
			
			var videoSelector = {video : true};
			if (window.navigator.appVersion.match(/Chrome\/(.*?) /)) {
				var chromeVersion = parseInt(window.navigator.appVersion.match(/Chrome\/(\d+)\./)[1], 10);
				if (chromeVersion < 20) {
					videoSelector = "video";
				}
			};
		
			navigator.getUserMedia(videoSelector, function( stream ) {
				if (vid.mozCaptureStream) {
					vid.mozSrcObject = stream;
				} else {
					vid.src = (window.URL && window.URL.createObjectURL(stream)) || stream;
				}
				vid.play();
			}, function() {
				//insertAltVideo(vid);
				alert("There was some problem trying to fetch video from your webcam. If you have a webcam, please make sure to accept when the browser asks for access to your webcam.");
			});
		} else {
			//insertAltVideo(vid);
			alert("This demo depends on getUserMedia, which your browser does not seem to support. :(");
		}

		vid.addEventListener('canplay', enablestart, false);
		
		// setup of emotion detection
		var ctrack2 = new clm.tracker({useWebGL : true});
		ctrack2.init(pModel);

		function startVideo() {
			// start video
			vid.play();
			// start tracking
			ctrack2.start(vid);
			// start loop to draw face
			drawLoop();
		}
		
		var initial = [0,0,0,0]
		var initial_window = 0
		var INITIAL_WINDOW_SIZE = 1
		var REFRESH_WINDOW_SIZE = 500
		var refresh_counter = -1 

		function drawLoop() {
			requestAnimFrame(drawLoop);
			var cp = ctrack2.getCurrentParameters();
			var er = ec.meanPredict(cp);
			if (er) {
				// Generate a baseline average so that 
				// Calibration sequence
				if (initial_window < INITIAL_WINDOW_SIZE){
					for (var i = 0;i < er.length;i++) {
						initial[i] += er[i].value/INITIAL_WINDOW_SIZE;
						initial_window += 1;
					}
				}
				// Actually find the emotion that is being recorded
				else {
					maxEmotion = "neutral";
					sad = 0;
					happy = 0;
					angry = 0;
					surprised = 0;

					maxValue = 0;
					for (var i = 0;i < er.length;i++) {
						// if (  (maxValue < (er[i].value-initial[i])/initial[i]) || er[i].value > 0.6 ){
						if ( maxValue < er[i].value ){
							maxEmotion = er[i].emotion;
							maxValue = er[i].value;
						}

						if (er[i].emotion == "happy"){
							// happy = Math.max((er[i].value - initial[i])/er[i].value, er[i].value);
							happy = er[i].value * 4;
							happyWindow.pop();
						    happyWindow.unshift(happy);
						}

						if (er[i].emotion == "sad"){
							sad = er[i].value;
							sadWindow.pop();
						    sadWindow.unshift(sad);
							// sad =  Math.max((er[i].value - initial[i])/er[i].value, er[i].value);
						}

						if (er[i].emotion == "angry"){
							angry  = er[i].value;
							angryWindow.pop();
						    angryWindow.unshift(angry);
							// =  Math.max((er[i].value - initial[i])/er[i].value, er[i].value);
						}

						if (er[i].emotion == "surprised"){
							surprised = er[i].value;
							surprisedWindow.pop();
						    surprisedWindow.unshift(surprised);
							 // =  Math.max((er[i].value - initial[i])/er[i].value, er[i].value);
						}

						$("." + er[i].emotion + "value").text(Math.round(er[i].value * 1000) / 1000);
					}

					// console.log(maxEmotion)
					
					// make this the default emotion that is displayed
					maxEmotion = "neutral"

					// average over the window to reduce noise, can apply a weighting function to it
					happy = happyWindow.reduce( (prev, curr) => prev + curr )/(EMOTION_WINDOW_SIZE);
					sad = sadWindow.reduce( (prev, curr) => prev + curr )/(EMOTION_WINDOW_SIZE);
					angry = angryWindow.reduce( (prev, curr) => prev + curr )/(EMOTION_WINDOW_SIZE);
					surprised = surprisedWindow.reduce( (prev, curr) => prev + curr )/(EMOTION_WINDOW_SIZE);

					// a threshold cascade to see if the emotion is extreme enough to
					// characterize with certainty, and then display
					if (sad < 0.1){ // cascaded because both of these are less sensitive and need to be 				   boosted if they are expressed
						if (angry > 0.3){
							maxEmotion = "angry";
							registerEmotion(maxEmotion);
							$(".emotionvalue").text(maxEmotion);
							document.getElementById("emoji").src = "img/" + maxEmotion + ".png";
						}
						else if (angry < 0.1) {
							maxEmotion = "sad";
							registerEmotion(maxEmotion);
							$(".emotionvalue").text(maxEmotion);
							document.getElementById("emoji").src = "img/" + maxEmotion + ".png";
						}
					}
					else if (angry > 0.8 && happy < 0.5) {
						maxEmotion = "angry";
						registerEmotion(maxEmotion);
						$(".emotionvalue").text(maxEmotion);
						document.getElementById("emoji").src = "img/" + maxEmotion + ".png";
					}
					else if (surprised > 0.2){
						maxEmotion = "surprised";
						registerEmotion(maxEmotion);
						$(".emotionvalue").text(maxEmotion);
						document.getElementById("emoji").src = "img/" + maxEmotion + ".png";
					}
					else if (happy > 0.02) {
						maxEmotion = "happy";
						registerEmotion(maxEmotion);
						$(".emotionvalue").text(maxEmotion);
						document.getElementById("emoji").src = "img/" + maxEmotion + ".png";
					}
					
					// Neutral
					else {
						$(".emotionvalue").text("neutral");
						document.getElementById("emoji").src = "img/neutral.png";
					}

					refresh_counter += 1;
					if (refresh_counter == REFRESH_WINDOW_SIZE){
						ctrack2.reset();
						ctrack2.start(vid);
						refresh_counter = -1;
						console.log("refreshed face rec")
					}
				}
			}
		}
		
		var ec = new emotionClassifier();
		ec.init(emotionModel);
		var emotionData = ec.getBlank();			
	</script>


</body>
</html>