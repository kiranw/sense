<!-- <!DOCTYPE html> -->
<html>
<head>
	<meta charset='utf-8'/>
	<meta name="viewport" content="width=device-width, initial-scale=1"/>
	<title>Sense | Integrated Emotion Detection </title>
	<link rel='stylesheet' href='https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css'/>
	<link rel='stylesheet' href='css/style.css'/>
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
	<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,500" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700" rel="stylesheet">
	

	<script>
		// getUserMedia only works over https in Chrome 47+, so we redirect to https. Also notify user if running from file.
		if (window.location.protocol == "file:") {
			alert("You seem to be running this example directly from a file. Note that these examples only work when served from a server or localhost due to canvas cross-domain restrictions.");
		} else if (window.location.hostname !== "localhost" && window.location.protocol !== "https:"){
			window.location.protocol = "https";
		}
	</script>
		
</head>
<body>

	<script src="js/utils.js"></script>
	<script src="js/clmtrackr.js"></script>
	<script src="models/model_pca_20_svm_emotionDetection.js"></script>
	<script src="js/Stats.js"></script>
	<script src="js/d3.min.js"></script>
	<script src="js/emotion_classifier.js"></script>
	<script src="js/emotionmodel.js"></script>

	<script src="js/webgazer.js" type="text/javascript"></script>


<div id="leftpanel">
	<!-- Collects the emojis that we annotate a page with -->
	<div id="emojiAnnotations"></div>

	<div class="title">User Test: Gaze Tracking</div>
	<div class="contentText">
		<p>First, make sure you calibrate the gaze tracking system by clicking on each corner and looking at it purposefully. After doing each corner twice, in circles, also make sure to calibrate the center and the corners once more.</p>

		<p>We display a blue dot on the screen - we want you to make eye contact with it, and press space bar. If it feels comfortable and natural, move the cursor where your eyes follow if that is how you read and use your computer when seeing content. Once, you press spacebar, the point will advance to the next position, and you will repeat the process.</p>

		<p>When complete, the data will automatically download.</p>
		<br><br>
		<p><a href="test2.html">Click here to go to the next user study!</a></p>	
	</div>
</div>


<div id="rightpanel">

	<img src="img/logo.png" class="logo" />

	<div id="content">
		<div id="controls">
			<input class="btn" type="button" value="wait, loading video" disabled="disabled" onclick="startVideo()" id="startbutton"></input>
		</div>
		<br>
		

		<div id="container">
			<video id="videoel" width="400" height="300" preload="auto" loop>
			</video>
			<canvas id="overlay" width="400" height="300"></canvas>
		</div>
	</div>
</div>


<!-- Shows where the smoothed gaze position is -->
<div id="eyePosPrediction"></div>




	




		<script>	
//===========================================================================
//===========================================================================
//            g a z e    t r a c k i n g   u s e r   t e s t
//===========================================================================
//===========================================================================
// make face, click spacebar
// put data in js object, etc.


function download(text, name, type) {
    var a = document.createElement("a");
    var file = new Blob([text], {type: type});
    a.href = URL.createObjectURL(file);
    a.download = name;
    a.click();
}

// Test 1-10
// var EYE_TEST_POS_X = [229,781,158,401,922,390,823,100,606,50]
// var EYE_TEST_POS_Y = [70,461,70,73,600,504,452,600,250,50]
var EYE_TEST_POS_X = [50];
var EYE_TEST_POS_Y = [50];
var results_X = [];
var results_Y = [];
current_test = 0
total_tests = 60;

$('#eyePosPrediction').css({
    transform: 'translate3d(' + EYE_TEST_POS_X[0] + 'px,' + EYE_TEST_POS_Y[0] + 'px,0)'
})  

// var gazeTest = {
// 	x_0: 0,
// 	y_0: 0,
// 	x_1: 0,
// 	y_1: 0,
// 	x_2: 0,
// 	y_2: 0,
// 	x_3: 0,
// 	y_3: 0,
// 	x_4: 0,
// 	y_4: 0,
// 	x_5: 0,
// 	y_5: 0,
// 	x_6: 0,
// 	y_6: 0,
// 	x_7: 0,
// 	y_7: 0,
// 	x_8: 0,
// 	y_8: 0,
// 	x_9: 0,
// 	y_9: 0
// }

document.body.onkeyup = function(e){
    if(e.keyCode == 32){
    	if (current_test < total_tests) {
	 		// gazeTest["x_"+current_test] = currentX;
	 		// gazeTest["y_"+current_test] = currentY;
	 		results_X.push(currentX);
	 		results_Y.push(currentY);

	 		current_test += 1;

	 		if (current_test != total_tests){
		 		nextX = Math.floor(Math.random() * 1200);
		 		nextY = Math.floor(Math.random() * 600);
		 		EYE_TEST_POS_X.push(nextX);
		 		EYE_TEST_POS_Y.push(nextY);


				$('#eyePosPrediction').css({
				    transform: 'translate3d(' + nextX + 'px,' + nextY + 'px,0)'
				})  	
	 		} 		
			else {
				file_text = JSON.stringify(EYE_TEST_POS_X) + "\n" + 
							JSON.stringify(EYE_TEST_POS_Y) + "\n" + 
							JSON.stringify(results_X) + "\n" + 
							JSON.stringify(results_Y);
				download(file_text, "gazeTest.txt", 'text/plain')
			}
		}
    }
    
}







//===========================================================================
//===========================================================================
//                        g a z e  t r a c k i n g
//===========================================================================
//===========================================================================

		var WINDOW_SIZE = 20;
		var EMOTION_TIMEOUT_MINIMUM = 200;
		var EMOTION_WINDOW_SIZE = 100;

		// Gaze Tracking constants and counters
		var gazeX = Array(WINDOW_SIZE).fill(0);
		var gazeY = Array(WINDOW_SIZE).fill(0);
		var currentX = 20;
		var currentY = 20;

		// Emotion window smoothing
		var happyWindow = Array(EMOTION_WINDOW_SIZE).fill(0);
		var sadWindow = Array(EMOTION_WINDOW_SIZE).fill(0);
		var surprisedWindow = Array(EMOTION_WINDOW_SIZE).fill(0);
		var angryWindow = Array(EMOTION_WINDOW_SIZE).fill(0);



		// Takes in the coordinates from WebGazer, adjusts them to be relative to viewports
		// and screen position, and then add our value to the smoothing protocol
		function updateGazePoints(x, y) {

			// Collect the scroll position because gaze tracking only gives us relative information based
			// on what is displayed on the page window
			// reference: http://stackoverflow.com/questions/3464876/javascript-get-window-x-y-position-for-scroll
			var topScrollOffset  = $("#leftpanel").scrollTop();
		    // var leftScrollOffset = window.pageXOffset || document.documentElement.scrollLeft;
		    var leftScrollOffset = 0;

			var xpred = x + leftScrollOffset; //these x coordinates are relative to the viewport 
		    var ypred = y + topScrollOffset; //these y coordinates are relative to the viewport

		    gazeX.pop();
		    gazeY.pop();
		    gazeX.unshift(xpred);
		    gazeY.unshift(ypred);

		    // average over the window to reduce noise, can apply a weighting function to it
			currentX = gazeX.reduce( (prev, curr) => prev + curr )/(WINDOW_SIZE);
			currentY = gazeY.reduce( (prev, curr) => prev + curr )/(WINDOW_SIZE);

			// display the results
		    // console.log(currentX, currentY);
			

		}

		


		// Set regression model used in tracking
		webgazer.setRegression('weightedRidge');

		// Collect prediction values
		webgazer.setGazeListener(function(data, elapsedTime) {
		    if (data == null) {
		        return;
		    }

		    // Bound prediction by viewport edges
		    webgazer.util.bound(data);
		    var xprediction = data.x; // These x coordinates are relative to the viewport 
		    var yprediction = data.y; // These y coordinates are relative to the viewport

		    // Add to window for smoothing
		    updateGazePoints(xprediction, yprediction);
		    
		}).begin()
		.showPredictionPoints(false);
		           


//===========================================================================
//===========================================================================
//            e m o t i o n    r e c o g n i t i o n 
//===========================================================================
//===========================================================================

		var vid = document.getElementById('videoel');
		// var overlay = document.getElementById('overlay');
		// var overlayCC = overlay.getContext('2d');
		
		/********** check and set up video/webcam **********/

		function enablestart() {
			var startbutton = document.getElementById('startbutton');
			startbutton.value = "start";
			startbutton.disabled = null;
		}
		
		navigator.getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.mozGetUserMedia || navigator.msGetUserMedia;
		window.URL = window.URL || window.webkitURL || window.msURL || window.mozURL;

		// check for camerasupport
		if (navigator.getUserMedia) {
			// set up stream
			
			var videoSelector = {video : true};
			if (window.navigator.appVersion.match(/Chrome\/(.*?) /)) {
				var chromeVersion = parseInt(window.navigator.appVersion.match(/Chrome\/(\d+)\./)[1], 10);
				if (chromeVersion < 20) {
					videoSelector = "video";
				}
			};
		
			navigator.getUserMedia(videoSelector, function( stream ) {
				if (vid.mozCaptureStream) {
					vid.mozSrcObject = stream;
				} else {
					vid.src = (window.URL && window.URL.createObjectURL(stream)) || stream;
				}
				vid.play();
			}, function() {
				//insertAltVideo(vid);
				alert("There was some problem trying to fetch video from your webcam. If you have a webcam, please make sure to accept when the browser asks for access to your webcam.");
			});
		} else {
			//insertAltVideo(vid);
			alert("This demo depends on getUserMedia, which your browser does not seem to support. :(");
		}

		vid.addEventListener('canplay', enablestart, false);
		
		// setup of emotion detection
		var ctrack2 = new clm.tracker({useWebGL : true});
		ctrack2.init(pModel);

		function startVideo() {
			// start video
			vid.play();
			// start tracking
			ctrack2.start(vid);
			// start loop to draw face
			drawLoop();
		}
		
		initial = [0,0,0,0]
		initial_window = 0
		INITIAL_WINDOW_SIZE = 1

		function drawLoop() {
			requestAnimFrame(drawLoop);
			var cp = ctrack2.getCurrentParameters();
			var er = ec.meanPredict(cp);
			if (er) {
				// Generate a baseline average so that 
				// Calibration sequence
				if (initial_window < INITIAL_WINDOW_SIZE){
					for (var i = 0;i < er.length;i++) {
						initial[i] += er[i].value/INITIAL_WINDOW_SIZE;
						initial_window += 1;
					}
				}
				// Actually find the emotion that is being recorded
				else {
					maxEmotion = "neutral";
					sad = 0;
					happy = 0;
					angry = 0;
					surprised = 0;

					maxValue = 0;
					for (var i = 0;i < er.length;i++) {
						// if (  (maxValue < (er[i].value-initial[i])/initial[i]) || er[i].value > 0.6 ){
						if ( maxValue < er[i].value ){
							maxEmotion = er[i].emotion;
							maxValue = er[i].value;
						}

						if (er[i].emotion == "happy"){
							// happy = Math.max((er[i].value - initial[i])/er[i].value, er[i].value);
							happy = er[i].value * 4;
							happyWindow.pop();
						    happyWindow.unshift(happy);
						}

						if (er[i].emotion == "sad"){
							sad = er[i].value;
							sadWindow.pop();
						    sadWindow.unshift(sad);
							// sad =  Math.max((er[i].value - initial[i])/er[i].value, er[i].value);
						}

						if (er[i].emotion == "angry"){
							angry  = er[i].value;
							angryWindow.pop();
						    angryWindow.unshift(angry);
							// =  Math.max((er[i].value - initial[i])/er[i].value, er[i].value);
						}

						if (er[i].emotion == "surprised"){
							surprised = er[i].value;
							surprisedWindow.pop();
						    surprisedWindow.unshift(surprised);
							 // =  Math.max((er[i].value - initial[i])/er[i].value, er[i].value);
						}

					}

					console.log(maxEmotion)
					
					// make this the default emotion that is displayed
					maxEmotion = "neutral"

					// average over the window to reduce noise, can apply a weighting function to it
					happy = happyWindow.reduce( (prev, curr) => prev + curr )/(EMOTION_WINDOW_SIZE);
					sad = sadWindow.reduce( (prev, curr) => prev + curr )/(EMOTION_WINDOW_SIZE);
					angry = angryWindow.reduce( (prev, curr) => prev + curr )/(EMOTION_WINDOW_SIZE);
					surprised = surprisedWindow.reduce( (prev, curr) => prev + curr )/(EMOTION_WINDOW_SIZE);

					// a threshold cascade to see if the emotion is extreme enough to
					// characterize with certainty, and then display
					if (sad < 0.1){ // cascaded because both of these are less sensitive and need to be 				   boosted if they are expressed
						if (angry > 0.4){
							maxEmotion = "angry";
							
						}
						else if (angry < 0.1) {
							maxEmotion = "sad";
							
						}
					}
					else if (surprised > 0.2){
						maxEmotion = "surprised";
						
					}
					else if (happy > 0.02) {
						maxEmotion = "happy";
						
					}
					else {
						maxEmotion = "neutral";
					}
				}
			}
		}
		
		var ec = new emotionClassifier();
		ec.init(emotionModel);
		var emotionData = ec.getBlank();			
	</script>


</body>
</html>